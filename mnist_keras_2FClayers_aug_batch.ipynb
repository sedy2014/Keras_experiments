{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST prediction with FC layers, and IMAGE Data generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "# set image dimension for Conv layer etc based on tensor flow or theano\n",
    "K.set_image_dim_ordering('tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load (downloaded if needed to : C:\\Users\\sidha\\.keras\\datasets\\mnist.npz) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape) # X_train.shape result is a tuple\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "# number of training samples\n",
    "N1 = X_train.shape[0]  # same as  N1= X_train.shape and then N1 = N1[0]\n",
    "N2 = X_test.shape[0]  \n",
    "h = X_train.shape[1]\n",
    "w = X_train.shape[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline Model with Multi-Layer Perceptrons\n",
    "\n",
    "#For a multi-layer perceptron model we must reduce the images down into a vector of pixels. \n",
    "#In this case the 28Ã—28 sized images will be 784 pixel input values.\n",
    "num_pixels = h*w\n",
    "# reshape N1 samples to num_pixels\n",
    "x_train = X_train.reshape(N1, num_pixels).astype('float32') # shape is now (60000,784)\n",
    "x_test = X_test.reshape(N2, num_pixels).astype('float32') # shape is now (10000,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, the output variable is an integer from 0 to 9. This is a multi-class classification problem. 10 digits \n",
    "# classified to 10 classes\n",
    "#As such, it is good practice to use a one hot encoding of the class values,\n",
    "#transforming the vector of class integers into a binary matrix.\n",
    "\n",
    "#We can easily do this using the built-in np_utils.to_categorical() helper function in Keras.\n",
    "y_train = np_utils.to_categorical(y_train) #(60000,10): 10000 lables for 10 classes\n",
    "y_test = np_utils.to_categorical(y_test) # (10000,10): 10000 lables for 10 classes\n",
    "num_classes = y_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "# create model\n",
    "    model = Sequential()\n",
    "    # Define input layerwhich is same as hidden layer with the same number of neurons as there are inputs (784). \n",
    "    # use RELU for this hidden layer\n",
    "    #model.add(Dense(784,input_shape=(N1,w,h,1), kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    # Define output layer with softmax function\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # Compile model\n",
    "    #  use ADAm optimizer and  Logarithmic loss or  categorical_crossentropy Loss\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "epochs = 20\n",
    "max_batches = 2 * len(x_train) / batch_size # 2*60000/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape to be [samples][width][height][ channel] for ImageDataGenerator\n",
    "x_t = X_train.reshape(N1, w, h, 1).astype('float32')\n",
    "datagen = ImageDataGenerator(rescale= 1./255)\n",
    "train_gen = datagen.flow(x_t, y_train, batch_size=batch_size)\n",
    "for e in range(epochs):\n",
    "    batches = 0\n",
    "    for x_batch, y_batch in train_gen:\n",
    "    # x_batch is of size [batch_sz,w,h,ch]: resize to [bth_sz,pixel_sz]: (200,28,28,1)-> (200,784)\n",
    "    # for model.fit\n",
    "        x_batch = np.reshape(x_batch, [-1, num_pixels])            \n",
    "        model.fit(x_batch, y_batch,validation_split=0.15,verbose=0)\n",
    "        batches += 1\n",
    "        print(\"Epoch %d/%d, Batch %d/%d\" % (e+1, epochs, batches, max_batches))\n",
    "        if batches >= max_batches:\n",
    "        # we need to break the loop by hand because\n",
    "        # the generator loops indefinitely\n",
    "            break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation of the model on test data( will predict to classes and also give error)\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Baseline Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF-GPU",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
